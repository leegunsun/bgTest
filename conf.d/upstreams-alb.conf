# Enhanced NGINX Upstream Configuration for ALB Integration
# 4 Application Instances per EC2 for Dual EC2 + AWS ALB Architecture

# Application Backend Pool (4 instances per EC2)
# Each instance runs on consecutive ports: 3001, 3002, 3003, 3004
upstream app_backend {
    # Load balancing method - least_conn for better distribution
    least_conn;
    
    # Application instances (4 per EC2)
    server 127.0.0.1:3001 max_fails=2 fail_timeout=30s weight=1;
    server 127.0.0.1:3002 max_fails=2 fail_timeout=30s weight=1;
    server 127.0.0.1:3003 max_fails=2 fail_timeout=30s weight=1;
    server 127.0.0.1:3004 max_fails=2 fail_timeout=30s weight=1;
    
    # Connection pooling for performance
    keepalive 32;
    keepalive_requests 1000;
    keepalive_timeout 60s;
}

# Health check specific upstream (for ALB health checks)
# Distributes health checks across all instances
upstream health_backend {
    # Round-robin for health checks
    server 127.0.0.1:3001 max_fails=1 fail_timeout=5s;
    server 127.0.0.1:3002 max_fails=1 fail_timeout=5s;
    server 127.0.0.1:3003 max_fails=1 fail_timeout=5s;
    server 127.0.0.1:3004 max_fails=1 fail_timeout=5s;
    
    # Minimal keepalive for health checks
    keepalive 8;
}

# Fallback upstream for emergency situations
# Routes to any available instance when primary upstream fails
upstream fallback_backend {
    # Backup configuration - tries instances in order
    server 127.0.0.1:3001 max_fails=5 fail_timeout=10s;
    server 127.0.0.1:3002 max_fails=5 fail_timeout=10s backup;
    server 127.0.0.1:3003 max_fails=5 fail_timeout=10s backup;
    server 127.0.0.1:3004 max_fails=5 fail_timeout=10s backup;
    
    keepalive 16;
}

# Development/Testing upstream for direct instance access
# Useful for debugging and testing individual instances
upstream instance_1 {
    server 127.0.0.1:3001 max_fails=1 fail_timeout=5s;
    keepalive 4;
}

upstream instance_2 {
    server 127.0.0.1:3002 max_fails=1 fail_timeout=5s;
    keepalive 4;
}

upstream instance_3 {
    server 127.0.0.1:3003 max_fails=1 fail_timeout=5s;
    keepalive 4;
}

upstream instance_4 {
    server 127.0.0.1:3004 max_fails=1 fail_timeout=5s;
    keepalive 4;
}

# Enhanced configurations for graceful shutdown support
# Map for deployment state-based routing
map $deployment_state $selected_upstream {
    default         app_backend;
    "draining"      health_backend;  # Use health backend during drain
    "maintenance"   fallback_backend;
    "debug"         instance_1;
}

# Map for upstream identification in headers
map $upstream_addr $upstream_instance {
    ~^127\.0\.0\.1:3001  "instance-1";
    ~^127\.0\.0\.1:3002  "instance-2";
    ~^127\.0\.0\.1:3003  "instance-3";
    ~^127\.0\.0\.1:3004  "instance-4";
    default              "unknown";
}

# Health check routing map
map $request_uri $health_upstream {
    default                   health_backend;
    "~^/health/instance/1"   instance_1;
    "~^/health/instance/2"   instance_2;
    "~^/health/instance/3"   instance_3;
    "~^/health/instance/4"   instance_4;
}

# Configuration notes for graceful shutdown:
# 1. To drain connections from a specific instance:
#    - Mark instance as 'down' in upstream: server 127.0.0.1:3001 down;
#    - Reload NGINX: nginx -s reload
#    - Wait for existing connections to finish (deregistration_delay period)
#    - Then safely restart the PM2 process
#
# 2. For complete application drain:
#    - Set deployment_state to 'draining' via environment variable
#    - This will route new requests to health endpoints only
#    - Existing connections will continue to work until they complete